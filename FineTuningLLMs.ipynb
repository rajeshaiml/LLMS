{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp6Zwg1oIcLPF8PB96tz2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshaiml/LLMS/blob/main/FineTuningLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare finetuned vs. non-finetuned models"
      ],
      "metadata": {
        "id": "vZ4jGIxAR0YN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSm_kWZZRKFS"
      },
      "outputs": [],
      "source": [
        "from llama import BasicModelRunner"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try Non-Finetuned models"
      ],
      "metadata": {
        "id": "BJexrlb9R4sO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_finetuned = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")"
      ],
      "metadata": {
        "id": "FcbgdiZnR6rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_finetuned_output = non_finetuned(\"Tell me how to train my dog to sit\")"
      ],
      "metadata": {
        "id": "b8dRPDDpR7UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned_output)"
      ],
      "metadata": {
        "id": "A4A2NkkQR9pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"What do you think of Mars?\"))"
      ],
      "metadata": {
        "id": "yEBLRWbMR--T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"taylor swift's best friend\"))"
      ],
      "metadata": {
        "id": "BlQnPcatSBzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "id": "L7oYCG0mSFQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare to finetuned models"
      ],
      "metadata": {
        "id": "TKx4PuOWSIRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")"
      ],
      "metadata": {
        "id": "FtgZDT7ISJHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_output = finetuned_model(\"Tell me how to train my dog to sit\")"
      ],
      "metadata": {
        "id": "oTdjp5cNSKzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_output)"
      ],
      "metadata": {
        "id": "5uxnYepmSMQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"[INST]Tell me how to train my dog to sit[/INST]\"))"
      ],
      "metadata": {
        "id": "SfB9Ta0nSSNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"[INST]Tell me how to train my dog to sit[/INST]\"))"
      ],
      "metadata": {
        "id": "oaHaja7OSSd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"What do you think of Mars?\"))"
      ],
      "metadata": {
        "id": "vzfH9wkFSSlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"taylor swift's best friend\"))"
      ],
      "metadata": {
        "id": "8BxcYduESStN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "id": "YlBD53tXSS1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare to ChatGPT"
      ],
      "metadata": {
        "id": "8s2c3CKFS3a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt = BasicModelRunner(\"chat-gpt\")"
      ],
      "metadata": {
        "id": "tJwIoOGtSS9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatgpt(\"Tell me how to train my dog to sit\"))"
      ],
      "metadata": {
        "id": "ue6F0_gbSTGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning data: compare to pretraining and basic preparation\n"
      ],
      "metadata": {
        "id": "0y6hkqIsTHcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "PdJrQdJYSTPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look at pretraining data set"
      ],
      "metadata": {
        "id": "gZDKsyUJTL0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sorry**, \"The Pile\" dataset is currently relocating to a new home and so we can't show you the same example that is in the video. Here is another dataset, the [\"Common Crawl\"](https://huggingface.co/datasets/c4) dataset."
      ],
      "metadata": {
        "id": "WuaFegp5TM2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pretrained_dataset = load_dataset(\"EleutherAI/pile\", split=\"train\", streaming=True)\n",
        "\n",
        "pretrained_dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n"
      ],
      "metadata": {
        "id": "vcpVAHQGSTWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "print(\"Pretrained dataset:\")\n",
        "top_n = itertools.islice(pretrained_dataset, n)\n",
        "for i in top_n:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "5DlCysrETTdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrast with company finetuning dataset you will be using"
      ],
      "metadata": {
        "id": "Oaj9nczhTX0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"lamini_docs.jsonl\"\n",
        "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
        "instruction_dataset_df"
      ],
      "metadata": {
        "id": "eiuP3bDUTTjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Various ways of formatting your data"
      ],
      "metadata": {
        "id": "-tWFObDjTbYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = instruction_dataset_df.to_dict()\n",
        "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "text"
      ],
      "metadata": {
        "id": "bc2lizjnTTxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"question\" in examples and \"answer\" in examples:\n",
        "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "elif \"instruction\" in examples and \"response\" in examples:\n",
        "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
        "elif \"input\" in examples and \"output\" in examples:\n",
        "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "else:\n",
        "  text = examples[\"text\"][0]"
      ],
      "metadata": {
        "id": "3chiPOJQTT3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_qa = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\n",
        "{answer}\"\"\""
      ],
      "metadata": {
        "id": "tL5bJbOrThOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = examples[\"question\"][0]\n",
        "answer = examples[\"answer\"][0]\n",
        "\n",
        "text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
        "text_with_prompt_template"
      ],
      "metadata": {
        "id": "HeG5aOUrTikX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_q = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\""
      ],
      "metadata": {
        "id": "ytGR0L68Tj3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = len(examples[\"question\"])\n",
        "finetuning_dataset_text_only = []\n",
        "finetuning_dataset_question_answer = []\n",
        "for i in range(num_examples):\n",
        "  question = examples[\"question\"][i]\n",
        "  answer = examples[\"answer\"][i]\n",
        "\n",
        "  text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
        "  finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
        "\n",
        "  text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
        "  finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": answer})"
      ],
      "metadata": {
        "id": "6h33Dc8KTl9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(finetuning_dataset_text_only[0])"
      ],
      "metadata": {
        "id": "BRzgcsmTTprD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(finetuning_dataset_question_answer[0])"
      ],
      "metadata": {
        "id": "pQ8XzU48TrRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common ways of storing your data"
      ],
      "metadata": {
        "id": "GUyGHerMTtnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(f'lamini_docs_processed.jsonl', 'w') as writer:\n",
        "    writer.write_all(finetuning_dataset_question_answer)"
      ],
      "metadata": {
        "id": "GBpuudNPTvuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
        "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
        "print(finetuning_dataset)"
      ],
      "metadata": {
        "id": "BEmRU7GeTwV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction-tuning"
      ],
      "metadata": {
        "id": "411zGVxNU5kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import jsonlines\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "from llama import BasicModelRunner\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "FkJjQq4lU6VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load instruction tuned dataset"
      ],
      "metadata": {
        "id": "-7g7Wt6uU-b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)"
      ],
      "metadata": {
        "id": "96mQ-TT1U7ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 5\n",
        "print(\"Instruction-tuned dataset:\")\n",
        "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
        "for j in top_m:\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "rciQ-DWjU7a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two prompt templates"
      ],
      "metadata": {
        "id": "mD_G-NgCVELJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ],
      "metadata": {
        "id": "s-tRVpvNU7OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hydrate prompts (add data to prompts)"
      ],
      "metadata": {
        "id": "dFH2ImmeVLBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data = []\n",
        "for j in top_m:\n",
        "  if not j[\"input\"]:\n",
        "    processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
        "  else:\n",
        "    processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
        "\n",
        "  processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})\n"
      ],
      "metadata": {
        "id": "t7Y6u5YIVL3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(processed_data[0])"
      ],
      "metadata": {
        "id": "dsEpsO14VOGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save data to jsonl\n"
      ],
      "metadata": {
        "id": "otAHFF7mVSTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
        "    writer.write_all(processed_data)"
      ],
      "metadata": {
        "id": "74MZ9BooVS9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare non-instruction-tuned vs. instruction-tuned models"
      ],
      "metadata": {
        "id": "ayDtCnm7VdLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path_hf = \"lamini/alpaca\"\n",
        "dataset_hf = load_dataset(dataset_path_hf)\n",
        "print(dataset_hf)"
      ],
      "metadata": {
        "id": "pAttIc71Vfdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
        "non_instruct_output = non_instruct_model(\"Tell me how to train my dog to sit\")\n",
        "print(\"Not instruction-tuned output (Llama 2 Base):\", non_instruct_output)"
      ],
      "metadata": {
        "id": "zJ4uaQctVhIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "instruct_output = instruct_model(\"Tell me how to train my dog to sit\")\n",
        "print(\"Instruction-tuned output (Llama 2): \", instruct_output)"
      ],
      "metadata": {
        "id": "0MavNKPUVhFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt = BasicModelRunner(\"chat-gpt\")\n",
        "instruct_output_chatgpt = chatgpt(\"Tell me how to train my dog to sit\")\n",
        "print(\"Instruction-tuned output (ChatGPT): \", instruct_output_chatgpt)"
      ],
      "metadata": {
        "id": "1hHIliB-VhCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try smaller models"
      ],
      "metadata": {
        "id": "PYqQYbZrVtZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
      ],
      "metadata": {
        "id": "llqlzHX9VhAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "CbqQw-4RVvBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
        "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
        "print(finetuning_dataset)"
      ],
      "metadata": {
        "id": "dLOmsCwvVvRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = finetuning_dataset[\"test\"][0]\n",
        "print(test_sample)\n",
        "\n",
        "print(inference(test_sample[\"question\"], model, tokenizer))"
      ],
      "metadata": {
        "id": "mXniqdOQVvK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare to finetuned small model"
      ],
      "metadata": {
        "id": "3iysm09IWD3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")"
      ],
      "metadata": {
        "id": "Y0JWB_JPV4PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inference(test_sample[\"question\"], instruction_model, tokenizer))"
      ],
      "metadata": {
        "id": "MqJ5bxhMV4AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pssst! If you were curious how to upload your own dataset to Huggingface\n",
        "# Here is how we did it\n",
        "\n",
        "# !pip install huggingface_hub\n",
        "# !huggingface-cli login\n",
        "\n",
        "# import pandas as pd\n",
        "# import datasets\n",
        "# from datasets import Dataset\n",
        "\n",
        "# finetuning_dataset = Dataset.from_pandas(pd.DataFrame(data=finetuning_dataset))\n",
        "# finetuning_dataset.push_to_hub(dataset_path_hf)"
      ],
      "metadata": {
        "id": "4_FHSGB1V38q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "rLtuJWSSWptD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "\n",
        "from pprint import pprint\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "17wLUZMwWsi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing text\n"
      ],
      "metadata": {
        "id": "UElO2q0eWvdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
      ],
      "metadata": {
        "id": "pysDcZzmWsfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hi, how are you?\""
      ],
      "metadata": {
        "id": "WJ0aYP29WsdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokenizer(text)[\"input_ids\"]"
      ],
      "metadata": {
        "id": "e7LBdT-XWsbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text"
      ],
      "metadata": {
        "id": "cGIV6eEBWsZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(\"Decoded tokens back into text: \", decoded_text)"
      ],
      "metadata": {
        "id": "NYR3gzOHWsWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize multiple texts at once"
      ],
      "metadata": {
        "id": "n7_n1jC_W8bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
        "encoded_texts = tokenizer(list_texts)\n",
        "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
      ],
      "metadata": {
        "id": "E6tMWNW8WsUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding and truncation"
      ],
      "metadata": {
        "id": "-MmlKSAlXAs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
        "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
      ],
      "metadata": {
        "id": "zD3tMRH6WsR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
        "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
      ],
      "metadata": {
        "id": "QdYes3GLWsPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.truncation_side = \"left\"\n",
        "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
        "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
      ],
      "metadata": {
        "id": "pBrkdlWCWsMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
        "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
      ],
      "metadata": {
        "id": "qMAUAnsxWsKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare instruction dataset"
      ],
      "metadata": {
        "id": "xWc7AuMWXNXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = \"lamini_docs.jsonl\"\n",
        "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
        "examples = instruction_dataset_df.to_dict()\n",
        "\n",
        "if \"question\" in examples and \"answer\" in examples:\n",
        "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "elif \"instruction\" in examples and \"response\" in examples:\n",
        "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
        "elif \"input\" in examples and \"output\" in examples:\n",
        "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "else:\n",
        "  text = examples[\"text\"][0]\n",
        "\n",
        "prompt_template = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "num_examples = len(examples[\"question\"])\n",
        "finetuning_dataset = []\n",
        "for i in range(num_examples):\n",
        "  question = examples[\"question\"][i]\n",
        "  answer = examples[\"answer\"][i]\n",
        "  text_with_prompt_template = prompt_template.format(question=question)\n",
        "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
        "\n",
        "from pprint import pprint\n",
        "print(\"One datapoint in the finetuning dataset:\")\n",
        "pprint(finetuning_dataset[0])"
      ],
      "metadata": {
        "id": "WMmIzRT_XPA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize a single example"
      ],
      "metadata": {
        "id": "88nLTu8PXTu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    padding=True\n",
        ")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "Zm84d0maXO9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 2048\n",
        "max_length = min(\n",
        "    tokenized_inputs[\"input_ids\"].shape[1],\n",
        "    max_length,\n",
        ")"
      ],
      "metadata": {
        "id": "9Kxak-DsXO7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    truncation=True,\n",
        "    max_length=max_length\n",
        ")"
      ],
      "metadata": {
        "id": "KIsyrjGMWsHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs[\"input_ids\"]"
      ],
      "metadata": {
        "id": "WJ_gkQYbWsEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the instruction dataset"
      ],
      "metadata": {
        "id": "iRN5vk-fXaiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    if \"question\" in examples and \"answer\" in examples:\n",
        "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "    elif \"input\" in examples and \"output\" in examples:\n",
        "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "    else:\n",
        "      text = examples[\"text\"][0]\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    max_length = min(\n",
        "        tokenized_inputs[\"input_ids\"].shape[1],\n",
        "        2048\n",
        "    )\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "FTjMaapYWsB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
        "\n",
        "tokenized_dataset = finetuning_dataset_loaded.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "EwSHMrWbWr5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "id": "LNvFmRYyWru3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare test/train splits"
      ],
      "metadata": {
        "id": "CAH7GE3jXfNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "print(split_dataset)"
      ],
      "metadata": {
        "id": "PV7cux30WrQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some datasets for you to try"
      ],
      "metadata": {
        "id": "i9VxzDjAXl0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
        "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
        "print(finetuning_dataset)"
      ],
      "metadata": {
        "id": "7C00hkvwXhfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
        "bts_dataset = \"lamini/bts\"\n",
        "open_llms = \"lamini/open_llms\""
      ],
      "metadata": {
        "id": "ErIWEtUsXhb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
        "print(dataset_swiftie[\"train\"][1])"
      ],
      "metadata": {
        "id": "Ej9zfzD1XhZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how to push your own dataset to your Huggingface hub\n",
        "# !pip install huggingface_hub\n",
        "# !huggingface-cli login\n",
        "# split_dataset.push_to_hub(dataset_path_hf)"
      ],
      "metadata": {
        "id": "x8sdED_uXhWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Bj_N0wXxaDl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technically, it's only a few lines of code to run on GPUs (elsewhere, ie. on Lamini).\n",
        "```\n",
        "from llama import BasicModelRunner\n",
        "\n",
        "model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
        "model.load_data_from_jsonlines(\"lamini_docs.jsonl\")\n",
        "model.train()\n",
        "```\n",
        "1. Choose base model.\n",
        "2. Load data.\n",
        "3. Train it. Returns a model ID, dashboard, and playground interface.\n",
        "\n",
        "### Let's look under the hood at the core code running this! This is the open core of Lamini's `llama` library :)"
      ],
      "metadata": {
        "id": "rpCSngXqaIMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import config\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from utilities import *\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "from llama import BasicModelRunner\n",
        "from llama import BasicModelRunner\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ],
      "metadata": {
        "id": "7x_2YtvxXhNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Lamini docs dataset"
      ],
      "metadata": {
        "id": "khe9zBBNaMWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"lamini_docs.jsonl\"\n",
        "dataset_path = f\"/content/{dataset_name}\"\n",
        "use_hf = False"
      ],
      "metadata": {
        "id": "08O9JzRDXhEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"lamini/lamini_docs\"\n",
        "use_hf = True"
      ],
      "metadata": {
        "id": "9NduKlwHXg4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the model, training config, and tokenizer"
      ],
      "metadata": {
        "id": "xJtcnaScaRls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/pythia-70m\""
      ],
      "metadata": {
        "id": "PhLgQuONXgSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_config = {\n",
        "    \"model\": {\n",
        "        \"pretrained_name\": model_name,\n",
        "        \"max_length\" : 2048\n",
        "    },\n",
        "    \"datasets\": {\n",
        "        \"use_hf\": use_hf,\n",
        "        \"path\": dataset_path\n",
        "    },\n",
        "    \"verbose\": True\n",
        "}"
      ],
      "metadata": {
        "id": "rpe4SehhV35t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "iQ1IPg-haWhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the base model"
      ],
      "metadata": {
        "id": "56rJawwraZHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "mVy3IDusaWeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    logger.debug(\"Select GPU device\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    logger.debug(\"Select CPU device\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "GJxCCngBaWbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.to(device)"
      ],
      "metadata": {
        "id": "oMmBI_dPaWY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to carry out inference"
      ],
      "metadata": {
        "id": "5XOV665hadH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "OpgOinYUaWV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try the base model"
      ],
      "metadata": {
        "id": "SsT2mZuqaffa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
        "print(\"Model's answer: \")\n",
        "print(inference(test_text, base_model, tokenizer))"
      ],
      "metadata": {
        "id": "NRATgqjMaWS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup training"
      ],
      "metadata": {
        "id": "l9La1mrLaiAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 3"
      ],
      "metadata": {
        "id": "aZmXqBlMaWPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
        "output_dir = trained_model_name"
      ],
      "metadata": {
        "id": "_FTBLx4daWL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=1,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=120, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "id": "r3ZF8dRaaWIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_flops = (\n",
        "  base_model.floating_point_ops(\n",
        "    {\n",
        "       \"input_ids\": torch.zeros(\n",
        "           (1, training_config[\"model\"][\"max_length\"])\n",
        "      )\n",
        "    }\n",
        "  )\n",
        "  * training_args.gradient_accumulation_steps\n",
        ")\n",
        "\n",
        "print(base_model)\n",
        "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
        "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
      ],
      "metadata": {
        "id": "oRNfNkIUaWES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    model_flops=model_flops,\n",
        "    total_steps=max_steps,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "hZPP92YRaV_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a few steps"
      ],
      "metadata": {
        "id": "652yQe-gasE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_output = trainer.train()"
      ],
      "metadata": {
        "id": "wrLevu2NaqQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save model locally"
      ],
      "metadata": {
        "id": "h2DWpnWHaupK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ],
      "metadata": {
        "id": "oODZ5asmaqOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
      ],
      "metadata": {
        "id": "W4oLMeUMaqK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model.to(device)\n"
      ],
      "metadata": {
        "id": "FNpjbjdaaqHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run slightly trained model"
      ],
      "metadata": {
        "id": "M_YbThVhay05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_question)\n",
        "\n",
        "print(\"Finetuned slightly model's answer: \")\n",
        "print(inference(test_question, finetuned_slightly_model, tokenizer))"
      ],
      "metadata": {
        "id": "x5wUf-OBaqCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_answer = test_dataset[0]['answer']\n",
        "print(\"Target answer output (test):\", test_answer)"
      ],
      "metadata": {
        "id": "EOKEULO-ap29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run same model trained for two epochs"
      ],
      "metadata": {
        "id": "JbWSVBXwa3Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "\n",
        "finetuned_longer_model.to(device)\n",
        "print(\"Finetuned longer model's answer: \")\n",
        "print(inference(test_question, finetuned_longer_model, tokenizer))"
      ],
      "metadata": {
        "id": "RwIEUr-Aa3-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run much larger trained model and explore moderation"
      ],
      "metadata": {
        "id": "uVxd21uia7NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
        "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
        "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
      ],
      "metadata": {
        "id": "p7qkB0m1a4VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(len(train_dataset)):\n",
        " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
        "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
        "  count += 1\n",
        "print(count)"
      ],
      "metadata": {
        "id": "ub1MRxFZa4R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore moderation using small model\n",
        "First, try the non-finetuned base model:"
      ],
      "metadata": {
        "id": "rcluXLucbAro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
      ],
      "metadata": {
        "id": "VmuhNC6Ba4LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now try moderation with finetuned small model"
      ],
      "metadata": {
        "id": "cRQfIIkObDpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
      ],
      "metadata": {
        "id": "uz44iUsda4IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune a model in 3 lines of code using Lamini"
      ],
      "metadata": {
        "id": "82Gt3QvlbGNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
        "model.load_data_from_jsonlines(\"lamini_docs.jsonl\")\n",
        "model.train(is_public=True) # -> returns an ID, dashboard, and chat interface"
      ],
      "metadata": {
        "id": "_2pBZDbGa4D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the link above. The training can take a few minutes. You may need to refresh the URL to detect the change from 'In Progress' to 'Completed'"
      ],
      "metadata": {
        "id": "_ku9bCTwbLRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:5b0e47de-93a9-4f8e-8cd7-a42de1ce4fe3.png)"
      ],
      "metadata": {
        "id": "_99iT0idbNKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:02381ae5-d062-4f87-b7fe-6ac45b7f4fdb.png)"
      ],
      "metadata": {
        "id": "WseMJcqFbOqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "dB3k-lcJbiC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Technically, there are very few steps to run it on GPUs, elsewhere (ie. on Lamini).\n",
        "```\n",
        "finetuned_model = BasicModelRunner(\n",
        "    \"lamini/lamini_docs_finetuned\"\n",
        ")\n",
        "finetuned_output = finetuned_model(\n",
        "    test_dataset_list # batched!\n",
        ")\n",
        "```\n",
        "\n",
        "### Let's look again under the hood! This is the open core code of Lamini's `llama` library :)"
      ],
      "metadata": {
        "id": "-AwisMDebka4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import config\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import difflib\n",
        "import pandas as pd\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from utilities import *\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ],
      "metadata": {
        "id": "-ub_88KRbI86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"lamini/lamini_docs\")\n",
        "\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "xYdSwJUmboJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset[0][\"question\"])\n",
        "print(test_dataset[0][\"answer\"])"
      ],
      "metadata": {
        "id": "5xFFvq5aboGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lamini/lamini_docs_finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "ahDyqk_wboDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup a really basic evaluation function"
      ],
      "metadata": {
        "id": "iyZrU9Yobstj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_exact_match(a, b):\n",
        "    return a.strip() == b.strip()"
      ],
      "metadata": {
        "id": "jycIo5NzboBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "vplejNHfbn-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  input_ids = tokenizer.encode(\n",
        "      text,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "8MbX2jZIbn7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run model and compare to expected answer"
      ],
      "metadata": {
        "id": "7rAWBksDbwkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[0][\"question\"]\n",
        "generated_answer = inference(test_question, model, tokenizer)\n",
        "print(test_question)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "id": "i9IXPsTzbn4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = test_dataset[0][\"answer\"]\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "k0G58R-5bn1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match = is_exact_match(generated_answer, answer)\n",
        "print(exact_match)"
      ],
      "metadata": {
        "id": "4lorZJgIb1JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run over entire dataset"
      ],
      "metadata": {
        "id": "dGujA53tb4Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "metrics = {'exact_matches': []}\n",
        "predictions = []\n",
        "for i, item in tqdm(enumerate(test_dataset)):\n",
        "    print(\"i Evaluating: \" + str(item))\n",
        "    question = item['question']\n",
        "    answer = item['answer']\n",
        "\n",
        "    try:\n",
        "      predicted_answer = inference(question, model, tokenizer)\n",
        "    except:\n",
        "      continue\n",
        "    predictions.append([predicted_answer, answer])\n",
        "\n",
        "    exact_match = is_exact_match(generated_answer, answer)\n",
        "    metrics['exact_matches'].append(exact_match)\n",
        "\n",
        "    if i > n and n != -1:\n",
        "      break\n",
        "print('Number of exact matches: ', sum(metrics['exact_matches']))"
      ],
      "metadata": {
        "id": "U4mYYMUOb1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "kk9yOYtAb1DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate all the data"
      ],
      "metadata": {
        "id": "25VLqbYZb7bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset_path = \"lamini/lamini_docs_evaluation\"\n",
        "evaluation_dataset = datasets.load_dataset(evaluation_dataset_path)"
      ],
      "metadata": {
        "id": "GClXbclEb1At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(evaluation_dataset)"
      ],
      "metadata": {
        "id": "-RHa3xo_b0-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try the ARC benchmark\n",
        "This can take several minutes"
      ],
      "metadata": {
        "id": "vF_Vzn35cB6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python lm-evaluation-harness/main.py --model hf-causal --model_args pretrained=lamini/lamini_docs_finetuned --tasks arc_easy --device cpu"
      ],
      "metadata": {
        "id": "LAhjIwu6b07o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zq2se6Abnys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ReC61vfibnv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JN5OAwhhbI5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yodJ-cQ_bI1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EcitM0kEbIvP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}